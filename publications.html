<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Selected Publications | Naveen Kuppuswamy</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>
<body>
    <header class="sticky-header">
        <div class="header-content">
            <div style="flex: 1;">
                <h1 class="name">Naveen Kuppuswamy</h1>
                <p class="blurb">Roboticist, Senior Research Scientist at <a href="https://www.tri.global">Toyota Research Institute</a></p>
                <div class="social-links">
                    <a href="mailto:naveen.sk@gmail.com">Email</a> / <a href="data/Naveen_CV.pdf">CV</a> / <a href="https://scholar.google.com/citations?user=Ec3-TsUAAAAJ">Scholar</a> / <a href="https://linkedin.com/in/naveen-kuppuswamy-5913808">LinkedIn</a>
                </div>
            </div>
            <div class="profile-thumb"><img src="images/IMG_0001.png"></div>
        </div>
        <nav class="nav-menu">
            <a href="index.html">About</a>
            <a href="projects.html">Projects</a>
            <a href="publications.html" class="active">Publications</a>
            <a href="media.html">Media</a>
        </nav>
    </header>

    <main class="main-wrap">
        <h2>Selected Publications</h2>
        
        <table class="paper-table">

            <tr>
                <td style="width:28%; vertical-align:middle;">
                    <div class="one">
                        <div class="two"><video muted autoplay loop playsinline><source src="images/polytouch_demo.mp4" type="video/mp4"></video></div>
                        <img src='images/polytouch_with_tool.jpg'>
                    </div>
                </td>
                <td style="width:72%; padding-left:20px; vertical-align:middle;">
                    <a href="https://polytouch.alanz.info/"><span class="papertitle">PolyTouch: A Robust Multi-Modal Tactile Sensor for Contact-rich Manipulation</span></a><br>
                    Jialiang Zhao, <strong>Naveen Kuppuswamy</strong>, Siyuan Feng, Benjamin Burchfiel, Edward Adelson<br>
                    <em>ICRA</em>, 2025 <span class="award">Best Paper Award, Field and Service Robotics</span><br>
                    <a href="https://polytouch.alanz.info/">project page</a> / <a href="https://arxiv.org/abs/2410.15059">arXiv</a>
                    <p>A robust multi-modal robot finger combining tactile, acoustic, and peripheral vision sensing for contact-rich manipulation. This work demonstrates the <strong>Contact Structure Principle</strong>, showing how rich haptic feedback fundamentally improves robot learning in complex environments.</p>
                </td>
            </tr>

            <tr>
                <td style="width:28%; vertical-align:middle;">
                    <div class="one">
                        <div class="two"><video muted autoplay loop playsinline><source src="images/apple.mp4" type="video/mp4"></video></div>
                        <img src='images/lbm_improvement.png'>
                    </div>
                </td>
                <td style="width:72%; padding-left:20px; vertical-align:middle;">
                    <a href="https://toyotaresearchinstitute.github.io/lbm1/"><span class="papertitle">A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation</span></a><br>
                    TRI LBM Team, Jose Barreiros, Andrew Beaulieu, Aditya Bhat, Rick Cory, Eric Cousineau, Hongkai Dai, Ching-Hsin Fang, Kunimatsu Hashimoto, Muhammad Zubair Irshad, Masha Itkina, <strong>Naveen Kuppuswamy</strong>, Kuan-Hui Lee, Katherine Liu, Dale McConachie, Ian McMahon, Haruki Nishimura, Calder Phillips-Grafflin, Charles Richter, Paarth Shah, Krishnan Srinivasan, Blake Wulfe, Chen Xu, Mengchao Zhang, Alex Alspach, Maya Angeles, Kushal Arora, Vitor Campagnolo Guizilini, Alejandro Castro, Dian Chen, Ting-Sheng Chu, Sam Creasey, Sean Curtis, Richard Denitto, Emma Dixon, Eric Dusel, Matthew Ferreira, Aimee Goncalves, Grant Gould, Damrong Guoy, Swati Gupta, Xuchen Han, Kyle Hatch, Brendan Hathaway, Allison Henry, Hillel Hochsztein, Phoebe Horgan, Shun Iwase, Donovon Jackson, Siddharth Karamcheti, Sedrick Keh, Joseph Masterjohn, Jean Mercat, Patrick Miller, Paul Mitiguy, Tony Nguyen, Jeremy Nimmer, Yuki Noguchi, Reko Ong, Aykut Onol, Owen Pfannenstiehl, Richard Poyner, Leticia Priebe Mendes Rocha, Gordon Richardson, Christopher Rodriguez, Derick Seale, Michael Sherman, Mariah Smith-Jones, David Tago, Pavel Tokmakov, Matthew Tran, Basile Van Hoorick, Igor Vasiljevic, Sergey Zakharov, Mark Zolotas, Rares Ambrus, Kerri Fetzer-Borelli, Benjamin Burchfiel, Hadas Kress-Gazit, Siyuan Feng, Stacie Ford, Russ Tedrake<br>
                    <em>arXiv</em>, 2025<br>
                    <a href="https://toyotaresearchinstitute.github.io/lbm1/">project page</a> / <a href="https://arxiv.org/abs/2507.05331">arXiv</a>
                    <p>A rigorous evaluation of Large Behavior Models (LBMs) through 1,800+ controlled real-world trials. Demonstrates that multitask pretraining on diverse robot data significantly improves robustness.</p>
                </td>
            </tr>

            <tr>
                <td style="width:28%; vertical-align:middle;">
                    <div class="one">
                        <div class="two"><video muted autoplay loop playsinline><source src="images/maniwav_demo.mp4" type="video/mp4"></video></div>
                        <img src='images/audio_attention.png'>
                    </div>
                </td>
                <td style="width:72%; padding-left:20px; vertical-align:middle;">
                    <a href="https://mani-wav.github.io/"><span class="papertitle">ManiWAV: Learning Robot Manipulation from In-the-Wild Audio-Visual Data</span></a><br>
                    Zeyi Liu, Cheng Chi, Eric Cousineau, <strong>Naveen Kuppuswamy</strong>, Benjamin Burchfiel, Shuran Song<br>
                    <em>CoRL</em>, 2024<br>
                    <a href="https://mani-wav.github.io/">project page</a> / <a href="https://arxiv.org/abs/2406.19464">arXiv</a>
                    <p>An "ear-in-hand" gripper design enabling robots to learn contact-rich manipulation from in-the-wild audio-visual demonstrations.</p>
                </td>
            </tr>

            <tr>
                <td style="width:28%; vertical-align:middle;">
                    <div class="one">
                        <div class="two"><video muted autoplay loop playsinline><source src="images/adaptive_compliance.mp4" type="video/mp4"></video></div>
                        <img src='images/adaptive_compliance.png'>
                    </div>
                </td>
                <td style="width:72%; padding-left:20px; vertical-align:middle;">
                    <a href="https://adaptive-compliance.github.io/"><span class="papertitle">Adaptive Compliance Policy: Learning Approximate Compliance for Diffusion Guided Control</span></a><br>
                    Yifan Hou, Zeyi Liu, Cheng Chi, Eric Cousineau, <strong>Naveen Kuppuswamy</strong>, Siyuan Feng, Benjamin Burchfiel, Shuran Song<br>
                    <em>ICRA</em>, 2025<br>
                    <a href="https://adaptive-compliance.github.io/">project page</a> / <a href="https://arxiv.org/abs/2410.14966">arXiv</a>
                    <p>A framework that learns to dynamically adjust robot compliance spatially and temporally from human demonstrations.</p>
                </td>
            </tr>

            <tr>
                <td style="width:28%; vertical-align:middle;">
                    <div class="one">
                        <div class="two"><video muted autoplay loop playsinline><source src="images/ic_best_practices.mp4" type="video/mp4"></video></div>
                        <img src='images/ic_best_practices.png'>
                    </div>
                </td>
                <td style="width:72%; padding-left:20px; vertical-align:middle;">
                    <a href="https://arxiv.org/abs/2409.09491"><span class="papertitle">Robot Learning as an Empirical Science: Best Practices for Policy Evaluation</span></a><br>
                    Hadas Kress-Gazit, Kunimatsu Hashimoto, <strong>Naveen Kuppuswamy</strong>, Paarth Shah, Phoebe Horgan, Gordon Richardson, Siyuan Feng, Benjamin Burchfiel<br>
                    <em>arXiv</em>, 2024<br>
                    <a href="https://arxiv.org/abs/2409.09491">arXiv</a>
                    <p>Proposes best practices for robot policy evaluation including detailed success criteria and failure mode characterization.</p>
                </td>
            </tr>

            <tr>
                <td style="width:28%; vertical-align:middle;">
                    <div class="one">
                        <div class="two"><video muted autoplay loop playsinline><source src="images/softbubble_demo.mp4" type="video/mp4"></video></div>
                        <img src='images/soft_bubble_intefact.jpg'>
                    </div>
                </td>
                <td style="width:72%; padding-left:20px; vertical-align:middle;">
                    <a href="https://ieeexplore.ieee.org/document/9341534"><span class="papertitle">Soft-bubble Grippers for Robust and Perceptive Manipulation</span></a><br>
                    <strong>Naveen Kuppuswamy</strong>, Alex Alspach, Avinash Uttamchandani, Sam Creasey, Takuya Ikeda, Russ Tedrake<br>
                    <em>IROS</em>, 2020<br>
                    <a href="https://punyo.tech/">project page</a> / <a href="https://arxiv.org/abs/2004.03691">arXiv</a>
                    <p>A highly-compliant bubble gripper combining soft robotics with dense geometry visuotactile sensing.</p>
                </td>
            </tr>

            <tr>
                <td style="width:28%; vertical-align:middle;">
                    <div class="one">
                        <div class="two"><video muted autoplay loop playsinline><source src="images/punyo_demo.mp4" type="video/mp4"></video></div>
                        <img src='images/punyo_1.png'>
                    </div>
                </td>
                <td style="width:72%; padding-left:20px; vertical-align:middle;">
                    <a href="https://arxiv.org/abs/2111.09354"><span class="papertitle">Punyo-1: Soft Tactile-Sensing Upper-Body Robot for Large Object Manipulation and Physical Human Interaction</span></a><br>
                    Aimee Goncalves, <strong>Naveen Kuppuswamy</strong>, Andrew Beaulieu, Avinash Uttamchandani, Katherine M. Tsui, Alex Alspach<br>
                    <em>RoboSoft</em>, 2022<br>
                    <a href="https://punyo.tech/">project page</a> / <a href="https://arxiv.org/abs/2111.09354">arXiv</a>
                    <p>A soft, tactile-sensing humanoid upper-body robot for whole-body rich-contact manipulation strategies.</p>
                </td>
            </tr>

        </table>
    </main>

    <footer style="text-align:right; padding: 40px; font-size: small; color: #999;">
        Website template from <a href="https://jonbarron.info/">Jon Barron</a>
    </footer>
</body>
</html>